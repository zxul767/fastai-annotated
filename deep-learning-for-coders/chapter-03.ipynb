{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style type='text/css'>\n",
       "  .rendered_html {\n",
       "      font-family: Courier New;\n",
       "      font-size:110%;\n",
       "      letter-spacing: 105%;\n",
       "      line-height:115%;\n",
       "  }\n",
       "  .rendered_html code {\n",
       "      font-size: 90%;\n",
       "  }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<style type='text/css'>\n",
    "  .rendered_html {\n",
    "      font-family: Courier New;\n",
    "      font-size:110%;\n",
    "      letter-spacing: 105%;\n",
    "      line-height:115%;\n",
    "  }\n",
    "  .rendered_html code {\n",
    "      font-size: 90%;\n",
    "  }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Data Ethics\n",
    "In this chapter we'll see no code or models, but we'll talk about a very important but often neglected topic: Data Ethics.\n",
    "\n",
    "As Jeremy (one of the co-authors of the course) admits, the topic of Ethics in general is complicated. Scholars in that discipline have a hard time agreeing on definitions of \"right\" and \"wrong\". Some authors, such as [Sam Harris](https://www.samharris.org/) in his book [The Moral Landscape](https://www.goodreads.com/book/show/7785194-the-moral-landscape), argue that even though it may not be within our current reach to have a science of morality, there is a strong argument based on our shared biology, that there may be objective ways to measure it.\n",
    "\n",
    "Nevertheless our inability to precisely establish the foundations of the field, there are practical concerns which most people would agree we have to address. That's the topic of this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biases in Vision Models\n",
    "\n",
    "![Biased Vision Models](https://user-images.githubusercontent.com/442314/176052086-b24f16aa-7bcf-438b-aad6-74c3c5cf41f7.jpg)\n",
    "\n",
    "![Google's Reaction](https://user-images.githubusercontent.com/442314/176052437-453c42d5-252f-4073-ba2e-481d1ad95701.jpg)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/442314/176052597-995d373b-ab8f-4dab-b426-5feddd340285.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biases in Language Models\n",
    "\n",
    "![Gender Bias in Language Models](https://user-images.githubusercontent.com/442314/176052739-b6791d65-e855-425f-9b5e-4a70f946a554.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Amplification in Classification Models\n",
    "\n",
    "![Gender Bias Amplification in Classification Models](https://user-images.githubusercontent.com/442314/176052974-5e84d4b9-93dd-42b4-8976-050487c14976.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Biases & Risks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When most laypeople hear about risks in ML (or more generally in AI), images of Hollywood movies such as The Terminator, I Robot, Ex-Machina, etc., immediately come to their minds.\n",
    "\n",
    "However, while there might be some points worth discussing about the themes in those movies, the more pressing issues we face today have nothing to do with such AGI doom scenarios. The series [Black Mirror](https://en.wikipedia.org/wiki/Black_Mirror) has several episodes which depict some of the more immediate issues we're facing as a result of the often careless deployment of narrow AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the most cited examples of risks in this regard are the following:\n",
    "\n",
    "+ **Racial/gender/... bias amplification**\n",
    "+ **Echo chambers creation** -- perhaps this can be grouped under the previous item, but it seems deserving of a separate mention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these risks stem from various biases that are latent in the deployment of ML systems. The most common ones include:\n",
    "\n",
    "+ **Historical Bias** -- this is inherent in our societies and accompanying values\n",
    "+ **Representation Bias** -- a methodology flaw that doesn't account for the reality of a population\n",
    "+ **Measurement Bias** -- the result of using flawed proxies to measure things we can't measure directly\n",
    "+ **Aggregation Bias** -- the exclusion of important/relevant information when building models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ML Biases](https://user-images.githubusercontent.com/442314/175117402-2679d178-bad7-47ac-928d-c3343be88438.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the above figure metnions two more kinds of biases (evaluation and deployment), I don't really think they qualify as biases but rather as risks.\n",
    "\n",
    "+ **Evaluation Risk** -- Is our evaluation data representative of the actual data in production?\n",
    "+ **Deployment Risk** -- Is it possible that our model will be used in unintended ways? Are we aware of the context in which it will operate?\n",
    "\n",
    "For a more thorough account of these risks and biases, see [A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle](https://arxiv.org/pdf/1901.10002.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Does Ethics provide a list of right answers?**\n",
    "\n",
    "No. Although there might be a science of morality (as some people suspect; see The Moral Landscape, by Sam Harris), we're far away from having objective ways to measure ethics and moral codes. \n",
    "\n",
    "But even though the frontiers between good and evil may be blurry and hard to define, perhaps we can say something more significant deeper into the regions on the issues that few people think are controversial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. How can working with people of different backgrounds help when considering ethical questions?**\n",
    "\n",
    "By sharing experiences, thoughts and feelings that are unique to their culture, upbringing and ideology, we may be able to better empathize with their needs and desires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What was the role of IBM in Nazi Germany? Why did the company participate as it did? Why did the workers participate?**\n",
    "\n",
    "It created machines to tabulate census data and perform logistics operations during WWWII.\n",
    "\n",
    "Its participation was most likely motivated by profit, thought there's some speculation that perhaps Watson was also aligned with Hithler's ideology.\n",
    "\n",
    "The workers participated simply out of inertia. \"I'm just taking orders\" or \"I'm just doing my job\" is the lazy cop-out for many people facing moral dilemmas. It's easier to find an excuse than to make difficult decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What was the role of the first person jailed in the Volkswagen diesel scandal?**\n",
    "\n",
    "He was one of the engineers working \"in the trenches\". One of the people who followed his boss's orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What was the problem with a database of suspected gang members maintained by California law enforcement officials?**\n",
    "\n",
    "It contained incorrect records of gang members who were babies at the time! To make matters worse, there was no appeal process in place to fix such blatant mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Why did YouTube's recommendation algorithm recommend videos of partially clothed children to pedophiles, even though no employee at Google had programmed this feature?**\n",
    "\n",
    "Because the algorithm may have been picking up on features that in isolation may be innocuous, but which can be troublesome when used in aggregation. In this case, features such as \"the video contains children\", \"partial nudity\", may have been used (beware this is mere speculation on my part).\n",
    "\n",
    "Keep in mind that the algorithm is simply trying to find common features in videos to try and recommend content to users. It has no common sense to determine whether such recommendations may have nefarious consequences. It is their creator's responsibility to watch and monitor carefully any unintended consequences of their creations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. What are the problems with the centrality of metrics?**\n",
    "\n",
    "One of the issues is that ignores how the model might end up being used. One example is models that optimize engagement in social mdedia. Ignoring how they will be deployed risks creating products that end up affecting people in their daily lives by creating subtle but powerful forms of addiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Why did Meetup.com not include gender in its recommendation system for tech meet ups?**\n",
    "\n",
    "To avoid a feedback loop that would amplify an existing bias: there is already an over representation of men in tech and this tool might recommend tech meetups to men assuming there's higher interest in that group, thus making the gap bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. What are the six types of bias in machine learning according to Suresh and Guttag?**\n",
    "\n",
    "+ **Historical Bias** -- this is inherent in our societies and accompanying values\n",
    "+ **Representation Bias** -- a methodology flaw that doesn't account for the reality of a population\n",
    "+ **Measurement Bias** -- the result of using flawed proxies to measure things we can't measure directly\n",
    "+ **Aggregation Bias** -- the exclusion of important/relevant information when building models\n",
    "\n",
    "The last two aren't really biases, but rather risks:\n",
    "\n",
    "+ **Evaluation Risk** -- Is our evaluation data representative of the actual data in production?\n",
    "+ **Deployment Risk** -- Is it possible that our model will be used in unintended ways? Are we aware of the context in which it will operate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Give two examples of historical race bias in the US.**\n",
    "\n",
    "Black people have often been thought of as being more prone to crime, leading to rampant discrimination. Asians (Japanese Americans in particular) were flagged during World War II as potential threats to national security of the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. Where are most images in ImageNet from?**\n",
    "\n",
    "The USA and Europe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. In the paper \"Does Machine Learning Automate Moral Hazard and Error?\" why is sinusitis found to be predictive of a stroke?**\n",
    "\n",
    "It's an error that conflates correlation with causation since the data collected corresponds to a small portion of all people who suffered a stroke, a population that happened to have access to healthcare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. What is representation bias?**\n",
    "\n",
    "It's a form of sampling bias. The data collectors didn't take the time to ensure the sample was representative of the population as a whole (either by empirical contrast or by use of statistical techniques.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14. How are machines and people different in terms of their use for making decisions?**\n",
    "\n",
    "Machines are often thought to be flawless, but people ignore that in the case of machine learning algorithms their output is a direct result of the training data which was created by humans and it doesn't take ill intent for this to go wrong. There are many kinds of biases that can wreak havoc. \n",
    "\n",
    "With humans there is less of an expectation of flawless decision-making and also often the opportunity to appeal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15. Is disinformation the same as \"fake news\"?**\n",
    "\n",
    "Not necessarily. Disinformation can be a more subtle mix of facts, half truths, deliberate lies, unconfirmed theories presented as facts, etc., sometimes meant to discourage people from seeking the truth by making the process very hard, or to polarize two or more groups and create social tension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16. Why is disinformation through auto generated text and particularly significant issue?**\n",
    "\n",
    "It's because deep learning models have become very good at creating text that looks as if written by humans, thus creating a weapon of disinformation at scale. People may no longer be able to easily discriminate facts from deliberate lies or to tell apart content that have ill intent (even if it contains truths in it). \n",
    "\n",
    "There is now an increasing need for digital signatures they have no centralized authority (perhaps use case for blockchain.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17. What are the five ethical lenses described by the Markkula Center?**\n",
    "\n",
    "+ The rights approach -- which options best respects the rights of all who have a stake?\n",
    "+ The justice approach -- which option treats people equally or proportionately?\n",
    "+ The utilitarian approach -- which option will produce the most good do the least harm?\n",
    "+ The common good approach -- which options best serves the community as a whole, not just some members?\n",
    "+ The virtue approach -- which option leads to me act as the sort of person I want to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**18. Where is policy an appropriate tool for addressing data ethics issues?**\n",
    "\n",
    "In issues where there are large misaligned incentives that will cause some to benefit largely while others will suffer the consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Read the article [\"What Happens When an Algorithm Cuts your Healthcare\"](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy). How could problems like this be avoided in the future?**\n",
    "\n",
    "The Drivetrain approach you described in this book could be extremely useful here, in particular the idea of human-monitored deployment to catch errors and issues early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Research to find out more about YouTube's recommendation system and its societal impact. Do you think recommendation systems must always have feedback loops with negative results? What approaches could Google take to avoid them? What about the government?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Read the paper [\"Discrimination in Online Ad Delivery\"](https://arxiv.org/pdf/1301.6822.pdf). Do you think Google should be considered responsible for what happened to Dr. Sweeney? what would be an appropriate response?**\n",
    "\n",
    "It's a bit hard to tell, as the author admits in the conclusions section. That's because we only have partial information about the inner workings of the systems and data involved (the texts for each ad).\n",
    "\n",
    "My conclusion from this partial information is that both Google and the advertisers bear some responsibility. The advertisers are clearly creating some clickbait text in their ads (\"Person X Arrested?\"), and Google is not imposing restrictions on that. \n",
    "\n",
    "Even in the case where the neutral texts were equal in proportion, Google should probably review the texts since the other economic dynamics of the ad auction may end up creating an imbalance in which ads are served to which users (replicating and amplifying existing human biases)\n",
    "\n",
    "You can see the paper [here]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. How can a cross-disciplinary team help avoid negative consequences?**\n",
    "\n",
    "The premise is that a plurality of opinions in a culturally diverse team will help find blindspots in a project that attempts to be fair for all stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Read the paper [\"Does Machine Learning Automate Moral Hazard and Error\"](https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf)? What actions do you think should be taken to deal with the issues identified in this paper?**\n",
    "\n",
    "The paper focuses on selection bias that results effectively in a form of measurement bias.\n",
    "\n",
    "For example, when attempting to build a model that predicts the probability of stroke, the training data may not contain a representative sample of all people who have had strokes, but merely of those who sought medical care and were thus recorded in hospital records. \n",
    "\n",
    "We can use this data, but any decisions we make based on the resulting models may have unexpected consequences. For instance, if it was used to allocate healthcare resources, we might be neglecting patients with different profiles to the ones in the training data.\n",
    "\n",
    "There's a few things we can do to deal with these issues:\n",
    "\n",
    "+ **Have subject matter experts that can validate whether the results of the model make sense.**\n",
    "In the paper, such an analysis reveals that there is a strong correlation between stroke and causes that don't seem to make much intuitive sense. This prompted the author to reason about the model and its training data, revealing the bias.\n",
    "\n",
    "\n",
    "+ **Create similar models with the same independent variables and an alternative but related dependent variable.**\n",
    "If the alternative dependent variable is less likely to be biased, we might be able to see different results in the coefficients for each independent variable (e.g., if we're using a linear regression model or any other model that we can query for variable importance). This is also a technique the author used in the paper to support the theory that the training data likely had the bias explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Read the article [\"How Will We Prevent AI-based Forgery?\"](https://hbr.org/2019/03/how-will-we-prevent-ai-based-forgery) Do you think Etzioni's proposed approach could work? Why?**\n",
    "\n",
    "The first question to consider is why ubiquitous adoption of existing anti-forgery schemes hasn't worked yet the way it has for e-commerce (e.g., `GNU PGP` tried very hard to promote the use of signed emails, but very few people seemed to care). \n",
    "\n",
    "I suspect that the answer is a combination of 1) the object in question is not valuable enough to warrant protection (as such protection is usually not hassle/cost free), 2) forgery may be more expensive than the potential gains in many cases, and 3) forgery can be relatively easy to spot. \n",
    "\n",
    "NLP models are poised to remove the last two barriers, so just one consideration remains. I suspect that, just as it happens with privacy today, two groups of users will emerge: casual Internet users who couldn't care less about forgery and will only take the option if it's built-in,  and a very vocal group who will care and proactively look for solutions: politicians, journalists, celebrities and anyone else whose public opinion matters.\n",
    "\n",
    "So, in conclusion, yes, I think it could work, but it will also require a shift in mindset for those who want to protect themselves against forgery. They will need to take their digital identity seriously, and hardware companies will have to find ways to also ensure that there are built-in mechanisms in devices (cameras, smartphones, etc.) to ensure the authenticity of pictures, sounds, videos, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
